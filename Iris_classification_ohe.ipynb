{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages\n",
    "* numpy - package for scientific computing with Python\n",
    "* seaborn - visualization library based on matplotlib, used here for importing the iris dataset.\n",
    "* train_test_split - Split arrays or matrices into random train and test subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import keras packages\n",
    "* Sequential - basic keras model composed of a linear stack of layers.\n",
    "* Dense - a regular densely-connected NN layer.\n",
    "* Activation - Activations can either be used through an Activation layer, or through the activation argument\n",
    "* np_utils - Converts a class vector (integers) to binary class matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the dataset\n",
    "* load the iris dataset from seaborn\n",
    "* explore the dataset using head and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris = sns.load_dataset(\"iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the dataset\n",
    "* split the dataset into x and y values, with all flower features in x and corresponding species names in y.\n",
    "* use the train_test_split method to split x an y randomly into train and test datasets. Argumets are :\n",
    "    * sequence of indexables with same length / shape.\n",
    "    * test_size : represent the proportion of the dataset to include in the test split.\n",
    "    * train_size : represent the proportion of the dataset to include in the train split.\n",
    "    * random_state : the seed used by the random number generator\n",
    "* perform one hot encoding on the categorical value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = iris.values[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = iris.values[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_X, test_data_X, train_data_y, test_data_y = \\\n",
    "    train_test_split(X, y, train_size=0.5, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform one-hot encoding\n",
    "* np.unique() - Returns the sorted unique elements of an array.\n",
    "* return_inverse - if True, returns the indices of the unique array that can be used to reconstruct the array.\n",
    "* to_categorical - Converts a class vector (integers) to binary class matrix. Arguments are class vector to be converted into a matrix and the total number of classes.\n",
    "* Identify the unique values and all the places in the array where each of them occurs. This will give us an array of integers with each unique value identified by a unique integer. Use the to_categorical method to replace the integer encoded variable with a new binary variable for each unique integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder(array):\n",
    "    unique_values, indices = np.unique(array, return_inverse=True)\n",
    "    \n",
    "    print(\"Unique values: \", unique_values)\n",
    "    print(\"Indices: \", indices)\n",
    "    \n",
    "    one_hot_encoded_data = np_utils.to_categorical(indices, len(unique_values))\n",
    "    \n",
    "    return one_hot_encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_train_data_y = one_hot_encoder(train_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ohe_test_data_y = one_hot_encoder(test_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ohe_train_data_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "* Define a sequential model\n",
    "* Add a dense layer with 16 neurons and relu activation. Set the input shape to 4 i.e. the number of feature inputs.\n",
    "* Add the output layer. Set the number of neurons to 3 which is the number of unique classes and the activation to softmax.  \n",
    "* Compile the model. Set the optimizer as adam and loss function as categorical_crossentropy as we're doing a classification and set metrics to accuracy to monitor the accuracy value over training.\n",
    "* Train the model ovber 10 epochs with a batch size of 2. Vary the number of epochs and batch size to tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(16, input_shape=(4,),name='Input_Layer',activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.add(Dense(3,name='Output_Layer',activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(train_data_X, ohe_train_data_y, epochs=10, batch_size=2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model accuracy\n",
    "* evaluate() - Returns the loss value & metrics values for the model in test mode.\n",
    "* Pass in the test data and its target labels for measuring the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_data_X, ohe_test_data_y, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Loss = {:.2f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy = {:.2f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a model to file\n",
    "* install h5py\n",
    "* serialize model to JSON\n",
    "* serialize weights to HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"saved_models/iris_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"saved_models/iris_model.h5\")\n",
    "print(\"Saved model to disk.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
